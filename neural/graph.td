graph TD
    A[Начало] --> B{Этап 1: Основа Нейронной Сети};

    B --> B1[Реализация NeuralNetworkLayer: Forward, Backward, Update];
    B1 --> B2[Тестирование: Юнит-тесты для матричных операций и функций активации];
    B2 --> B3[Тестирование: NeuralNetworkLayer на простых примерах];
    B3 --> B4[Реализация NeuralNetwork: Predict, Train, Clone];
    B4 --> B5[Тестирование: NeuralNetwork на задаче XOR и переобучение на маленьком батче];

    B{Этап 1: Основа Нейронной Сети} --> C{Этап 2: Логика Игры Крестики-Нолики};

    C --> C1[Реализация Board и методов: MakeMove, CheckWin, GetStateVector и т.д.];
    C1 --> C2[Тестирование: Ручные прогоны игры, проверка GetReward];

    C{Этап 2: Логика Игры Крестики-Нолики} --> D{Этап 3: Компоненты DQN};

    D --> D1[Реализация Experience и ReplayBuffer: Add, Sample];
    D1 --> D2[Тестирование: Добавление/выборка из буфера, проверка вытеснения];

    D{Этап 3: Компоненты DQN} --> E{Этап 4: Интеграция Агента DQN};

    E --> E1[Реализация DQNAgent: NewDQNAgent, ChooseAction, Train];
    E1 --> E2[Тестирование: ChooseAction с разным Epsilon, Train на синтетических данных];

    E{Этап 4: Интеграция Агента DQN} --> F{Этап 5: Основной Цикл Обучения};

    F --> F1[Настройка main(): Параметры, игровой цикл, вызовы Train, обновление TargetNetwork];
    F1 --> F2[Мониторинг: Отслеживание побед/поражений, Epsilon, (опционально) Loss];

    F{Этап 5: Основной Цикл Обучения} --> G{Этап 6: Оптимизация и Улучшение};

    G --> G1[Настройка гиперпараметров];
    G1 --> G2[Реализация более сложных оптимизаторов (Adam, RMSprop)];
    G2 --> G3[Обучение через Self-Play (агент против себя)];
    G3 --> G4[Тонкая настройка и развертывание];

    G{Этап 6: Оптимизация и Улучшение} --> H[Завершение];